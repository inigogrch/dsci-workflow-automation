---
title: "U.S. Adult Census: Income Prediction with Logistic Regression"
author: 
  - Benjamin Gerochi
  - Izzy Zhou
  - Michael Tham
  - Yui Mikuriya
format: 
    html:
        toc: true
        number-sections: true
        embed-resources: true
        fig-numbering: true
    pdf:
        toc: true
        number-sections: true
        fig-numbering: true
bibliography: references.bib
execute: 
    echo: false
    warning: false
editor: source
---


## Summary

This report investigates income prediction using the [UCI Adult Dataset](https://archive.ics.uci.edu/dataset/2/adult) [@kohavi1996], which compiles demographic and income data from the 1994 U.S. Census. The primary objective is to predict whether an individual earns over $50,000 annually using factors such as age, education level, and hours worked per week. By employing a logistic regression model, the analysis effectively predicted income levels on test cases while assessing model performance using metrics like the ROC curve (AUC: Area Under the Curve), sensitivity, specificity, and accuracy. The findings underscore that while the model achieves robust overall accuracy, there are challenges with false positives that warrant further refinement.

The insights derived from this study not only validate the role of education and work intensity in income determination but also suggest avenues for future research, such as integrating geographic and intersectional demographic variables to capture the complexities of income disparities. Overall, the analysis offers a comprehensive approach to understanding income inequality and provides actionable information for policy makers and individuals aiming to navigate economic opportunities.

## Introduction
### Dataset Overview

The dataset selected for this project is the [UCI Adult Dataset](https://archive.ics.uci.edu/dataset/2/adult) [@kohavi1996], available through the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml) [@uci_machine_learning_repository]. It contains demographic and income data collected by the **U.S. Census Bureau** and is widely used for predicting whether an individual’s income exceeds **$50,000 per year** based on various demographic factors.

### Dataset Details:
- **Dataset Name**: [UCI Adult Dataset](https://archive.ics.uci.edu/dataset/2/adult) [@kohavi1996]
- **Source**: 1994 U.S. Census database, compiled by Ronny Kohavi and Barry Becker  
- **Total Observations**: 32,561 
- **Total Variables**: 15


### Variables and Their Types

Table: Variable Index and Descriptions {#tbl-variables}

| Variable Index | Variable Name       | Type      | Description |
|----------------|---------------------|-----------|-------------|
| 0              | age                 | continuous       | Age of the individual |
| 1              | workclass           | categorical    | Employment sector |
| 2              | fnlwgt              | continuous       | Number of individuals represented |
| 3              | education           | categorical    | Highest level of education attained |
| 4              | education-num       | continuous       | Numerical version of education level |
| 5              | marital-status      | categorical    | Marital status |
| 6              | occupation          | categorical    | Type of occupation |
| 7              | relationship        | categorical    | Position in household |
| 8              | race                | categorical    | Race of the individual |
| 9              | sex                 | categorical    | Gender (Male/Female) |
| 10             | capital-gain        | continuous       | Capital gains earned |
| 11             | capital-loss        | continuous       | Capital losses incurred |
| 12             | hours-per-week      | continuous       | Average hours worked per week |
| 13             | native-country      | categorical    | Country of origin |
| 14             | income              | categorical    | Income level (<=50K, >50K) |


This [dataset](https://archive.ics.uci.edu/dataset/2/adult) includes both **categorical** and **numerical** variables, making it suitable for analyzing relationships between **demographic attributes** and **income levels**. Further **exploration and preprocessing** may involve handling **missing values** and **encoding categorical features**.  

### Research Question  
**How accurately can key demographic factors predict whether an individual's annual income exceeds $50,000?**  

This study aims to use demographic variables to predict income levels without pre-assuming key predictors. Our team initially analyzed different aspects of the dataset before deciding to focus on demographic influences on income such as age, education, and hours worked.  

### Literature Context  
Prior research supports the importance of demographic factors in income prediction. Jo [@jo2023] analyzed the **Adult dataset** and identified **capital gain, education, relationship status, and occupation** as key predictors. Similarly, Azzollini et al. [@azzollini2023] found that demographic differences explained **40% of income inequality** across OECD countries, reinforcing the relevance of our analysis.  

### **Objective**

To develop and evaluate a predictive model that estimates the probability of an individual earning more than $50,000 annually based on their demographic characteristics:

- **Prediction:** Build a robust model to forecast whether an individual's annual income will exceed $50,000.
- **Model Evaluation:** Assess model performance to ensure that the model provides reliable predictions.

## Methods & Results

### Loading the Libraries and Dataset
We will start by importing the necessary R libraries for data analysis and preprocessing. We then load the [dataset](https://archive.ics.uci.edu/dataset/2/adult) into R by referencing the downloaded file path. 

```{r}
#| vscode: {languageId: r}

library(broom)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(knitr)
library(pROC)
```

```{r}
#| label: tbl-raw-adult-dataset
#| tbl-cap: "Raw Adult Income Dataset"


income <- read_csv("../data/raw/adult_raw.csv")
knitr::kable(head(income))
```


### Data Wrangling

We will begin by cleaning @tbl-raw-adult-dataset. First, we remove missing values and convert the income column into a factor variable to ensure R treats it as a categorical variable. This transformation is crucial for statistical modeling and visualization, especially when income is used as a binary outcome in logistic regression. We also create new column names to streamline readability and analysis.

```{r}
#| label: tbl-cleaned-adult-dataset
#| tbl-cap: "Cleaned Adult Income Dataset"


income_clean <- read_csv("../data/clean/adult_clean.csv")
n_obs <- nrow(income_clean)
sample_size <- round(0.1 * nrow(income_clean))
knitr::kable(head(income_clean))
```
**Number of Rows: `{r} n_obs`**

After removing missing values from @tbl-cleaned-adult-dataset, we randomly sample 10% of the data (which contains a total of `{r} n_obs` observations), bringing our sample size to `{r} sample_size` data points. The sample is then split into training and testing sets (80-20 split) for prediction analysis.

```{r}
#| label: tbl-training-adult-dataset
#| tbl-cap: "Training Set of Adult Dataset"


income_training <- read_csv("../data/clean/adult_training.csv")
n_training <- nrow(income_training)
knitr::kable(head(income_training))
```
**Number of Rows: `{r} n_training`**

Above, we can see that @tbl-cleaned-adult-dataset has been successfully split, with @tbl-training-adult-dataset containing `{r} n_training` rows, representing about 80% of our sample size: `{r} sample_size`.

### Exploratory Data Analysis (EDA)

#### Pairwise Plot:

To focus on the most relevant variables, we will exclude columns that do not directly contribute to addressing our research question. Hence, we have retained demographic predictors such as age, education level, and hours worked per week. These predictors were chosen based on prior literature [@azzollini2023, @smithedgell2024], theoretical considerations, and empirical evidence from exploratory analyses, which indicate that they have a significant influence on income levels.

Using @tbl-training-adult-dataset with the irrelevant columns dropped, we create pairwise plots to examine relationships between continuous variables (`age`, `hours_per_week`, `education_num`) and the response variable, as well as associations among the input variables.

::: {.text-center}
![Pairwise Plot of Response and Predictors](../results/eda/pairwise_plot.png){#fig-pairwise-plot width=70% height=70%}
:::


@fig-pairwise-plot shows that `age` is right-skewed, `hours_per_week` peaks around 40, and `education_num` has a bimodal distribution. Weak correlations (< 0.6) suggest minimal multicollinearity.

The following code generates summary tables for continuous variables, with the code computing key summary statistics: mean, standard deviation, median, variance, maximum, and minimum. 

```{r}
#| label: tbl-summary-statistics
#| tbl-cap: "Summary Statistics Table of Relevant Predictors"


summary_statistics_table <- read_csv("../results/eda/summary_statistics.csv")
# Age stats
age_stats <- summary_statistics_table %>% filter(name == "age")
age_mean <- age_stats$mean
age_sd <- age_stats$sd
age_min <- age_stats$min
age_max <- age_stats$max
# Education stats
edu_stats <- summary_statistics_table %>% filter(name == "education_num")
edu_mean <- edu_stats$mean
edu_sd <- edu_stats$sd
# Hours stats
hours_stats <- summary_statistics_table %>% filter(name == "hours_per_week")
hours_mean <- hours_stats$mean
hours_sd <- hours_stats$sd
hours_max <- hours_stats$max

knitr::kable(summary_statistics_table)
```


@tbl-summary-statistics shows that the average age is `{r} round(age_mean, 0)` years (SD = `{r} round(age_sd, 2)`) with a range of `{r} age_min` to `{r} age_max`. The average education level (education_num) is `{r} round(edu_mean, 0)` years (SD = `{r} round(edu_sd, 2)`), reflecting high school or some college education. For hours_per_week, the average is `{r} round(hours_mean, 2)` hours (SD = `{r} round(hours_sd, 2)`), with a maximum of `{r} hours_max` hours, indicating some individuals work significantly long hours.

### Proposed Method: Logistic Regression and ROC Curve

Why is Logistic Regression Appropriate?

Logistic regression is suitable for modeling binary outcomes like income categories (<=50K and >50K). It estimates the probability of an individual falling into a specific category based on predictors, then classifies the predictions based on a threshold. The ROC Curve, on the other hand, is a reliable tool for evaluating the effectiveness of our model.

#### Assumptions:
1. Independence of observations.
2. No high correlation among predictors.
3. A large enough sample size for reliable estimates.

#### Limitations:
1. Potential underfitting if too little predictors are included.

### Fit the Logistic Regression Model
In the following code, we fit the logistic regression model to @tbl-training-adult-dataset using the relevant predictors. 

```{r}
#| label: tbl-logreg-summary
#| tbl-cap: "Summary of the Logistic Regression Model"


income_training <- read_csv("../data/clean/adult_training.csv")
log_reg_model <- readRDS("../results/model/log_reg_model.RDS")

actual_classes <- income_training$income
predicted_probs <- predict(log_reg_model, type = "response")
roc_curve <- roc(actual_classes, predicted_probs)
auc_value <- auc(roc_curve)
tidy_logreg <- broom::tidy(log_reg_model)
edu_coef <- tidy_logreg[tidy_logreg$term == "education_num", "estimate"]

knitr::kable(tidy_logreg)
```

We can observe from @tbl-logreg-summary that all predictors were deemed significant (based on the p-values). Furthermore, education number seemed to have the highest coefficient (`{r} edu_coef`), demonstrating the greatest impact on model predictions.

### Visualizing the ROC Curve
To evaluate the model, we will use the ROC curve to visualize the trade-off between sensitivity and specificity across classification thresholds. The AUC (Area Under the Curve) will be calculated to quantify model performance, with values closer to 1 indicating strong discrimination and values near 0.5 suggesting random guessing.

::: {.text-center}
![ROC Curve of the Logistic Regression Model](../results/model/roc_plot.png){#fig-roc-curve width=70% height=70%}
:::


@fig-roc-curve shows us that the AUC (Area Under the Curve) values obtained for the model (`{r} round(auc_value, 4)`) is significantly above 0.5, indicating that the model performs much better than random guessing. The high AUC value suggests that the model has strong discriminatory power, effectively distinguishing between individuals earning `<=50K` and `>50K` based on the selected predictors. 

### Test the Model on the Testing Dataset
Now, we perform the classification analysis and apply the model to the testing dataset and visualize the results of the analysis in a confusion matrix.

::: {.text-center}
![Confusion Matrix of Full Model on Testing Set](../results/model/conf_matrix_plot.png){#fig-confusion-plot width=70% height=70%}
:::


### Classification Results and Model Metrics

```{r}
#| label: tbl-classification-results
#| tbl-cap: "Classification Results and Model Metrics"


metrics_df <- read_csv("../results/model/summary_statistics.csv")
sensitivity <- metrics_df$Value[metrics_df$Metric == "Sensitivity"] * 100
specificity <- metrics_df$Value[metrics_df$Metric == "Specificity"] * 100
precision <- metrics_df$Value[metrics_df$Metric == "Precision"] * 100
accuracy <- metrics_df$Value[metrics_df$Metric == "Accuracy"] * 100
kappa <- metrics_df$Value[metrics_df$Metric == "Cohen's Kappa"] * 100
knitr::kable(metrics_df)
```

From @tbl-classification-results, we observe the following metrics:

1. **Sensitivity (SN): `{r} round(sensitivity,2)`%** - The model correctly identifies `{r} round(sensitivity,2)`% of higher-income individuals.

2. **Specificity (SP): `{r} round(specificity,2)`%** - `{r} round(specificity,2)`% of lower-income individuals are correctly classified.

3. **Precision (PR): `{r} round(precision,2)`%** - `{r} round(precision,2)`% of predicted >50K individuals actually earn >50K, indicating many false positives.

4. **Accuracy (ACC): `{r} round(accuracy,2)`%** - `{r} round(accuracy,2)`% of overall predictions are correct.

5. **Cohen's Kappa (κ): `{r} round(kappa,2)`%** - Moderate agreement, better than random chance but room for improvement.

### Interpretation
- Strong specificity, but low sensitivity and moderate precision suggest improvements in identifying high-income individuals.
- High accuracy reflects solid overall performance but overlooks class imbalance.
- Low Cohen's Kappa indicates the need for refinement to improve consistency of predictions.

## Discussion
### Summary of Findings and Implications

- The logistic regression model showed strong predictive power **(AUC =`{r} auc_value`)**, demonstrating that the model can effectively distinguish income levels better than a baseline.
- These findings can inform policies aimed at reducing income inequality. Education and hours worked were key predictors, emphasizing the need for skill development and work-life balance.
- Understanding the factors behind income disparities can help individuals make more informed career decisions and pursue opportunities for skill enhancement.

### Expectations and Results

- The model’s **AUC (`{r} auc_value`)** is strong, reflecting the importance of predictors like age, education, and hours worked. Overall, the results are consistent with expectations from the research study:
  - **Age** correlates with experience, leading to higher salaries.
  - **Education** increases income, with those holding a degree earning significantly more.
  - **Hours Worked** reflects labor input, where more hours can translate to higher pay.

### Future Research

- **Geographic Influence on Income:** Including geographic variables may reveal regional disparities in income linked to education and job opportunities.
- **Intersectionality of Demographics:** Exploring how race, gender, and marital status interact could improve the model's accuracy in predicting income.
- **Health and Disability Status:** Accounting for health conditions or disability could provide additional insight into income disparities by limiting education or work opportunities.

## References